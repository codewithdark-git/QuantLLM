# ğŸ§  QuantLLM: Lightweight Library for Quantized LLM Fine-Tuning and Deployment

[![PyPI Downloads](https://static.pepy.tech/badge/quantllm)](https://pepy.tech/projects/quantllm)
<img alt="PyPI - Version" src="https://img.shields.io/pypi/v/quantllm?logo=pypi&label=version&">


## ğŸ“Œ Overview

**QuantLLM** is a Python library designed for developers, researchers, and teams who want to fine-tune and deploy large language models (LLMs) **efficiently** using **4-bit and 8-bit quantization** techniques. It provides a modular and flexible framework for:

- **Loading and quantizing models** with advanced configurations
- **LoRA / QLoRA-based fine-tuning** with customizable parameters
- **Dataset management** with preprocessing and splitting
- **Training and evaluation** with comprehensive metrics
- **Model checkpointing** and versioning
- **Hugging Face Hub integration** for model sharing

The goal of QuantLLM is to **democratize LLM training**, especially in low-resource environments, while keeping the workflow intuitive, modular, and production-ready.

## ğŸ¯ Key Features

| Feature                          | Description |
|----------------------------------|-------------|
| âœ… Quantized Model Loading       | Load any HuggingFace model in 4-bit or 8-bit precision with customizable quantization settings |
| âœ… Advanced Dataset Management   | Load, preprocess, and split datasets with flexible configurations |
| âœ… LoRA / QLoRA Fine-Tuning      | Memory-efficient fine-tuning with customizable LoRA parameters |
| âœ… Comprehensive Training        | Advanced training loop with mixed precision, gradient accumulation, and early stopping |
| âœ… Model Evaluation             | Flexible evaluation with custom metrics and batch processing |
| âœ… Checkpoint Management        | Save, resume, and manage training checkpoints with versioning |
| âœ… Hub Integration              | Push models and checkpoints to Hugging Face Hub with authentication |
| âœ… Configuration Management     | YAML/JSON config support for reproducible experiments |
| âœ… Logging and Monitoring       | Comprehensive logging and Weights & Biases integration |

## ğŸš€ Getting Started

### Installation

```bash
pip install quantllm
```

For detailed usage examples and API documentation, please refer to our:
- ğŸ“š [Official Documentation](https://quantllm.readthedocs.io/)
- ğŸ“ [Tutorials](https://quantllm.readthedocs.io/tutorials/)
- ğŸ“– [API Reference](https://quantllm.readthedocs.io/api/)

## ğŸ’» Hardware Requirements

### Minimum Requirements
- **CPU**: 4+ cores
- **RAM**: 16GB
- **Storage**: 20GB free space
- **Python**: 3.8+

### Recommended Requirements
- **GPU**: NVIDIA GPU with 8GB+ VRAM
- **RAM**: 32GB
- **Storage**: 50GB+ SSD
- **CUDA**: 11.7+

### Resource Usage Guidelines
| Model Size | 4-bit (GPU RAM) | 8-bit (GPU RAM) | CPU RAM (min) |
|------------|----------------|-----------------|---------------|
| 3B params  | ~6GB          | ~9GB           | 16GB         |
| 7B params  | ~12GB         | ~18GB          | 32GB         |
| 13B params | ~20GB         | ~32GB          | 64GB         |
| 70B params | ~90GB         | ~140GB         | 256GB        |

## ğŸ”„ Version Compatibility

| QuantLLM | Python | PyTorch | Transformers | CUDA  |
|----------|--------|----------|--------------|-------|
| 0.1.x    | â‰¥3.8   | â‰¥2.0.0   | â‰¥4.30.0     | â‰¥11.7 |
| 0.2.x    | â‰¥3.9   | â‰¥2.1.0   | â‰¥4.31.0     | â‰¥11.8 |

## ğŸ—º Roadmap

- [ ] Multi-GPU training support
- [ ] AutoML for hyperparameter tuning
- [ ] More quantization methods
- [ ] Custom model architecture support
- [ ] Enhanced logging and visualization
- [ ] Model compression techniques
- [ ] Deployment optimizations

## ğŸ¤ Contributing

We welcome contributions! Please see our [CONTRIBUTE.md](CONTRIBUTE.md) for guidelines and setup instructions.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [HuggingFace](https://huggingface.co/) for their amazing Transformers library
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for quantization
- [PEFT](https://github.com/huggingface/peft) for parameter-efficient fine-tuning
- [Weights & Biases](https://wandb.ai/) for experiment tracking

## ğŸ“« Contact & Support

- GitHub Issues: [Create an issue](https://github.com/yourusername/QuantLLM/issues)
- Documentation: [Read the docs](https://quantllm.readthedocs.io/)
- Discord: [Join our community](https://discord.gg/quantllm)
- Email: support@quantllm.ai
