# ğŸš€ Quick Start

Get up and running with QuantLLM in 5 minutes.

---

## Your First Model

```python
from quantllm import turbo

# Load any HuggingFace model with automatic optimization
model = turbo("meta-llama/Llama-3.2-3B")

# Generate text
response = model.generate("Explain machine learning in simple terms")
print(response)
```

**That's it!** QuantLLM automatically:
- âœ… Detects your GPU and available memory
- âœ… Applies optimal 4-bit quantization
- âœ… Enables Flash Attention 2 when available
- âœ… Configures memory management

---

## Basic Usage

### Generate Text

```python
response = model.generate(
    "Write a Python function to calculate fibonacci numbers",
    max_new_tokens=200,
    temperature=0.7,
)
print(response)
```

### Chat Mode

```python
messages = [
    {"role": "system", "content": "You are a helpful coding assistant."},
    {"role": "user", "content": "How do I read a file in Python?"},
]

response = model.chat(messages, max_new_tokens=200)
print(response)
```

### Streaming Output

```python
for token in model.generate("Count to 10:", stream=True):
    print(token, end="", flush=True)
```

---

## Export to Different Formats

### GGUF (llama.cpp, Ollama, LM Studio)

```python
# Export with recommended Q4_K_M quantization
model.export("gguf", "model.Q4_K_M.gguf", quantization="Q4_K_M")

# Other quantization options
model.export("gguf", "model.Q8_0.gguf", quantization="Q8_0")   # Higher quality
model.export("gguf", "model.Q2_K.gguf", quantization="Q2_K")   # Smallest size
```

### ONNX (ONNX Runtime, TensorRT)

```python
model.export("onnx", "./model-onnx/")
```

### MLX (Apple Silicon)

```python
model.export("mlx", "./model-mlx/", quantization="4bit")
```

### SafeTensors (HuggingFace)

```python
model.export("safetensors", "./model-hf/")
```

---

## Fine-Tune Your Model

Train with your own data in one line:

```python
# Simple training
model.finetune("training_data.json", epochs=3)

# With more control
model.finetune(
    "training_data.json",
    epochs=5,
    learning_rate=2e-4,
    lora_r=16,
    batch_size=4,
)
```

**Supported data formats:**

```json
[
  {"instruction": "What is Python?", "output": "Python is a programming language..."},
  {"text": "Full text for language modeling"},
  {"prompt": "Question here", "completion": "Answer here"}
]
```

---

## Push to HuggingFace

Share your model with the world:

```python
# Push with auto-generated model card
model.push(
    "your-username/my-awesome-model",
    format="gguf",
    quantization="Q4_K_M",
    license="apache-2.0"
)
```

The model card includes:
- âœ… Proper YAML frontmatter for HuggingFace
- âœ… Format-specific usage examples
- âœ… "Use this model" button compatibility
- âœ… Quantization details

---

## Configuration Options

### Override Auto-Detection

```python
model = turbo(
    "meta-llama/Llama-3.2-3B",
    bits=4,                    # Force 4-bit quantization
    max_length=4096,           # Context length
    device="cuda:0",           # Specific GPU
    dtype="bfloat16",          # Data type
)
```

### View Current Configuration

```python
print(model.config)
```

---

## Load GGUF Models

Load pre-quantized GGUF models directly:

```python
from quantllm import TurboModel

model = TurboModel.from_gguf(
    "TheBloke/Llama-2-7B-Chat-GGUF",
    filename="llama-2-7b-chat.Q4_K_M.gguf"
)

print(model.generate("Hello!"))
```

---

## Show the Banner

Display the QuantLLM banner anytime:

```python
import quantllm

quantllm.show_banner()
```

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘   ğŸš€ QuantLLM v2.0.0                                       â•‘
â•‘   Ultra-fast LLM Quantization & Export                     â•‘
â•‘                                                            â•‘
â•‘   âœ“ GGUF  âœ“ ONNX  âœ“ MLX  âœ“ SafeTensors                     â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Next Steps

Now that you know the basics, explore more:

- [Loading Models â†’](guide/loading-models.md) â€” Advanced model loading options
- [Text Generation â†’](guide/generation.md) â€” Generation parameters and modes
- [GGUF Export â†’](guide/gguf-export.md) â€” All quantization types explained
- [Fine-tuning â†’](guide/finetuning.md) â€” Training with LoRA
- [Hub Integration â†’](guide/hub-integration.md) â€” Push and pull from HuggingFace
- [API Reference â†’](api/turbomodel.md) â€” Full API documentation
