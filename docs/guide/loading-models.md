# ðŸ“¥ Loading Models

QuantLLM provides flexible model loading with automatic optimization.

---

## Basic Loading

### The `turbo()` Function

The simplest way to load any model:

```python
from quantllm import turbo

# Load from HuggingFace Hub
model = turbo("meta-llama/Llama-3.2-3B")

# Load from local path
model = turbo("./my-local-model/")
```

### What Happens Automatically

When you call `turbo()`, QuantLLM:

1. **Detects your hardware** â€” GPU memory, CUDA version, capabilities
2. **Chooses quantization** â€” 4-bit for most GPUs, 8-bit for high-memory systems
3. **Enables optimizations** â€” Flash Attention 2, gradient checkpointing
4. **Configures memory** â€” Automatic offloading if needed

---

## Quantization Options

### Automatic (Recommended)

```python
# Let QuantLLM choose the best quantization
model = turbo("meta-llama/Llama-3.2-3B")
```

### Manual Bit-Width

```python
# Force specific quantization
model = turbo("meta-llama/Llama-3.2-3B", bits=4)   # 4-bit (smallest)
model = turbo("meta-llama/Llama-3.2-3B", bits=8)   # 8-bit (balanced)
model = turbo("meta-llama/Llama-3.2-3B", bits=16)  # FP16 (highest quality)
```

### Disable Quantization

```python
# Load in full precision (requires more memory)
model = turbo("meta-llama/Llama-3.2-3B", quantize=False)
```

---

## Configuration Options

### Common Options

```python
model = turbo(
    "meta-llama/Llama-3.2-3B",
    bits=4,                      # Quantization bits (4, 8, 16)
    max_length=4096,             # Maximum context length
    device="cuda:0",             # Device (cuda, cpu, auto)
    dtype="bfloat16",            # Data type (float16, bfloat16)
    trust_remote_code=True,      # For custom model architectures
    verbose=True,                # Show loading progress
)
```

### Memory Options

```python
model = turbo(
    "meta-llama/Llama-3.2-3B",
    bits=4,
    device_map="auto",           # Automatic device mapping
    low_cpu_mem_usage=True,      # Reduce CPU memory during loading
)
```

---

## Using TurboModel Directly

For more control, use the `TurboModel` class:

```python
from quantllm import TurboModel, SmartConfig

# Create custom config
config = SmartConfig.detect("meta-llama/Llama-3.2-3B", bits=4)

# Load with custom config
model = TurboModel.from_pretrained(
    "meta-llama/Llama-3.2-3B",
    config=config,
)
```

---

## Load GGUF Models

Load pre-quantized GGUF models directly from HuggingFace:

```python
from quantllm import TurboModel

# From HuggingFace Hub
model = TurboModel.from_gguf(
    "TheBloke/Llama-2-7B-Chat-GGUF",
    filename="llama-2-7b-chat.Q4_K_M.gguf"
)

# From local file
model = TurboModel.from_gguf("./models/my-model.gguf")
```

### List Available GGUF Files

```python
files = TurboModel.list_gguf_files("TheBloke/Llama-2-7B-Chat-GGUF")
print(files)
# ['llama-2-7b-chat.Q2_K.gguf', 'llama-2-7b-chat.Q4_K_M.gguf', ...]
```

---

## Supported Models

QuantLLM supports **45+ model architectures**:

| Family | Models |
|--------|--------|
| **Llama** | Llama 2, Llama 3, Llama 3.1, Llama 3.2, CodeLlama |
| **Mistral** | Mistral 7B, Mixtral 8x7B, Mixtral 8x22B |
| **Qwen** | Qwen, Qwen2, Qwen2.5, Qwen2-MoE |
| **Microsoft** | Phi-1, Phi-2, Phi-3 |
| **Google** | Gemma, Gemma 2 |
| **Falcon** | Falcon 7B, 40B, 180B |
| **Code Models** | StarCoder, StarCoder2, CodeGen |
| **Chinese** | ChatGLM, Yi, Baichuan, InternLM |
| **Other** | DeepSeek, StableLM, MPT, BLOOM, OPT, GPT-NeoX |

---

## Memory Optimization

### For Large Models

```python
# Enable gradient checkpointing (for training)
model = turbo("meta-llama/Llama-3-70B", bits=4)

# Use CPU offloading
model = turbo(
    "meta-llama/Llama-3-70B",
    bits=4,
    device_map="auto",  # Automatic CPU/GPU split
)
```

### Memory Usage Estimates

| Model Size | 4-bit | 8-bit | FP16 |
|------------|-------|-------|------|
| 3B | ~2 GB | ~4 GB | ~6 GB |
| 7B | ~4 GB | ~8 GB | ~14 GB |
| 13B | ~8 GB | ~14 GB | ~26 GB |
| 70B | ~40 GB | ~70 GB | ~140 GB |

---

## Best Practices

1. **Start with automatic settings** â€” Let QuantLLM detect your hardware
2. **Use 4-bit for most cases** â€” Best balance of quality and memory
3. **Check memory first** â€” `turbo()` shows memory stats before loading
4. **Use GGUF for inference** â€” Pre-quantized GGUF models load faster

---

## Next Steps

- [Text Generation â†’](generation.md) â€” Generate text with your model
- [Fine-tuning â†’](finetuning.md) â€” Train with your own data
- [GGUF Export â†’](gguf-export.md) â€” Export for deployment
